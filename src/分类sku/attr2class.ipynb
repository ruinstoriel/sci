{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 475,
   "id": "7783786bd438721",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T19:45:06.362023Z",
     "start_time": "2025-01-09T19:45:05.183920Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2499,) (2499, 166)\n",
      "0.9983993597438976\n",
      "0.8502581755593803\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  1010010001       0.78      0.88      0.82         8\n",
      "  1010010002       0.50      1.00      0.67         1\n",
      "  1010010003       1.00      1.00      1.00         2\n",
      "  1010020001       0.83      0.71      0.77         7\n",
      "  1010020002       0.75      1.00      0.86         3\n",
      "  1010020003       1.00      1.00      1.00         2\n",
      "  1010150001       1.00      0.75      0.86         8\n",
      "  1010150002       0.50      0.75      0.60         4\n",
      "  1010150003       0.00      0.00      0.00         0\n",
      "  1020010004       1.00      0.83      0.91         6\n",
      "  1020010005       1.00      1.00      1.00         6\n",
      "  1020010006       1.00      1.00      1.00         4\n",
      "  1020020004       1.00      1.00      1.00         2\n",
      "  1020020005       1.00      1.00      1.00         5\n",
      "  1020020006       0.86      1.00      0.92         6\n",
      "  1020150004       0.00      0.00      0.00         0\n",
      "  1020150005       1.00      0.91      0.95        11\n",
      "  1020150006       1.00      1.00      1.00         6\n",
      "  1030010007       1.00      0.81      0.89        21\n",
      "  1030010008       0.00      0.00      0.00         0\n",
      "  1030010009       0.50      0.50      0.50         2\n",
      "  1030010010       0.83      1.00      0.91         5\n",
      "  1030020007       1.00      0.94      0.97        18\n",
      "  1030020008       1.00      1.00      1.00         1\n",
      "  1030020009       0.00      0.00      0.00         0\n",
      "  1030020010       0.75      1.00      0.86         3\n",
      "  1030150007       1.00      1.00      1.00        24\n",
      "  1030150010       1.00      1.00      1.00         3\n",
      "  1040010011       0.89      0.73      0.80        11\n",
      "  1040010012       0.87      0.90      0.89        30\n",
      "  1040010013       1.00      1.00      1.00         8\n",
      "  1040020011       0.80      0.89      0.84         9\n",
      "  1040020012       0.94      0.91      0.92        32\n",
      "  1040020013       0.67      0.67      0.67         6\n",
      "  1040030012       0.00      0.00      0.00         1\n",
      "  1040150011       1.00      0.88      0.93         8\n",
      "  1040150012       0.91      0.91      0.91        32\n",
      "  1040150013       1.00      1.00      1.00         6\n",
      "  1050010014       0.00      0.00      0.00         0\n",
      "  1050010015       0.00      0.00      0.00         0\n",
      "  1050020014       0.00      0.00      0.00         0\n",
      "  1050020015       0.00      0.00      0.00         0\n",
      "  1050150014       0.00      0.00      0.00         0\n",
      "  1050150015       1.00      1.00      1.00         1\n",
      "  1070010017       1.00      1.00      1.00         7\n",
      "  1070010109       0.00      0.00      0.00         0\n",
      "  1070020017       1.00      1.00      1.00         5\n",
      "  1070020109       1.00      1.00      1.00         1\n",
      "  1070150017       0.00      0.00      0.00         0\n",
      "  1070150109       0.00      0.00      0.00         0\n",
      "  1080010018       1.00      1.00      1.00         1\n",
      "  1080010019       0.89      0.94      0.92        18\n",
      "  1080010020       1.00      0.94      0.97        18\n",
      "  1080010021       0.62      0.83      0.71         6\n",
      "  1080020018       0.00      0.00      0.00         0\n",
      "  1080020019       0.95      0.95      0.95        21\n",
      "  1080020020       0.92      0.92      0.92        13\n",
      "  1080020021       0.50      0.33      0.40         3\n",
      "  1080030019       0.00      0.00      0.00         0\n",
      "  1080150018       1.00      1.00      1.00         2\n",
      "  1080150019       0.88      1.00      0.93        21\n",
      "  1080150020       0.73      1.00      0.84         8\n",
      "  1080150021       1.00      0.83      0.91         6\n",
      "  1100010016       1.00      0.86      0.92         7\n",
      "  1100020016       1.00      1.00      1.00         7\n",
      "  1100150016       1.00      1.00      1.00         7\n",
      "  1110010026       0.67      0.33      0.44         6\n",
      "  1110010027       0.00      0.00      0.00         0\n",
      "  1110020026       0.75      1.00      0.86         3\n",
      "  1110020027       0.00      0.00      0.00         0\n",
      "  1110150026       0.64      0.78      0.70         9\n",
      "  1110150027       0.00      0.00      0.00         0\n",
      "  1130010022       1.00      1.00      1.00         1\n",
      "  1130010023       1.00      1.00      1.00         6\n",
      "  1130010024       0.95      0.97      0.96        38\n",
      "  1130010025       1.00      0.75      0.86         4\n",
      "  1130020022       1.00      1.00      1.00         2\n",
      "  1130020023       1.00      1.00      1.00         6\n",
      "  1130020024       0.79      1.00      0.88        15\n",
      "  1130020025       0.75      1.00      0.86         3\n",
      "  1130150022       1.00      1.00      1.00         1\n",
      "  1130150023       1.00      1.00      1.00         6\n",
      "  1130150024       0.97      1.00      0.98        28\n",
      "  1130150025       1.00      0.75      0.86         4\n",
      "  2010650046       0.00      0.00      0.00         0\n",
      "  2012250042       0.00      0.00      0.00         0\n",
      "  2012250043       1.00      1.00      1.00         4\n",
      "  2012250044       0.00      0.00      0.00         0\n",
      "  2012250045       0.00      0.00      0.00         0\n",
      "  2020120107       1.00      1.00      1.00         5\n",
      "  2020300105       1.00      1.00      1.00         1\n",
      "  2020650028       0.00      0.00      0.00         0\n",
      "  2020650029       0.00      0.00      0.00         0\n",
      "  2020650030       0.00      0.00      0.00         0\n",
      "  2020650031       0.00      0.00      0.00         0\n",
      "  2020650032       0.00      0.00      0.00         0\n",
      "  2020650033       0.00      0.00      0.00         0\n",
      "  2020650034       1.00      0.80      0.89         5\n",
      "  2020650035       0.00      0.00      0.00         0\n",
      "  2020650036       0.00      0.00      0.00         0\n",
      "  2020650037       1.00      1.00      1.00         3\n",
      "  2020650038       0.00      0.00      0.00         0\n",
      "  2030180098       0.00      0.00      0.00         0\n",
      "  2030400091       1.00      0.92      0.96        12\n",
      "  2030400099       1.00      1.00      1.00        14\n",
      "  2032250092       1.00      1.00      1.00         4\n",
      "  2032250093       1.00      0.92      0.96        12\n",
      "  2032250094       0.57      1.00      0.73         4\n",
      "  2032250095       0.00      0.00      0.00         0\n",
      "  2032250096       1.00      1.00      1.00         8\n",
      "  2032250097       0.00      0.00      0.00         0\n",
      "  2032250106       0.00      0.00      0.00         0\n",
      "  2040100059       0.00      0.00      0.00         0\n",
      "  2040180058       0.00      0.00      0.00         0\n",
      "  2040200041       0.00      0.00      0.00         0\n",
      "  2040400048       1.00      1.00      1.00        17\n",
      "  2040400050       1.00      1.00      1.00         2\n",
      "  2040400052       0.82      1.00      0.90         9\n",
      "  2040400054       1.00      1.00      1.00         5\n",
      "  2040400055       0.00      0.00      0.00         0\n",
      "  2040400057       0.00      0.00      0.00         0\n",
      "  2040400061       1.00      1.00      1.00         2\n",
      "  2042250039       1.00      1.00      1.00        12\n",
      "  2042250040       1.00      1.00      1.00        10\n",
      "  2042250041       0.00      0.00      0.00         0\n",
      "  2042250047       0.00      0.00      0.00         0\n",
      "  2042250049       0.00      0.00      0.00         0\n",
      "  2042250051       0.00      0.00      0.00         2\n",
      "  2042250053       1.00      1.00      1.00         4\n",
      "  2042250056       1.00      0.75      0.86         4\n",
      "  2042250060       1.00      0.80      0.89         5\n",
      "  2042250090       1.00      1.00      1.00        15\n",
      "  2070100108       0.88      1.00      0.93         7\n",
      "  2080100073       0.00      0.00      0.00         0\n",
      "  2080180068       0.00      0.00      0.00         0\n",
      "  2080180069       0.00      0.00      0.00         0\n",
      "  2080650067       0.80      0.80      0.80         5\n",
      "  2082250063       0.00      0.00      0.00         0\n",
      "  2082250064       0.84      1.00      0.92        27\n",
      "  2082250065       1.00      0.67      0.80        15\n",
      "  2082250066       0.00      0.00      0.00         0\n",
      "  2082250070       0.00      0.00      0.00         0\n",
      "  2082250071       0.80      1.00      0.89         4\n",
      "  2082250072       0.88      0.94      0.91        16\n",
      "  2100650062       1.00      1.00      1.00         6\n",
      "  2100650100       0.00      0.00      0.00         0\n",
      "  2110250076       1.00      1.00      1.00         5\n",
      "  2112250074       1.00      1.00      1.00         6\n",
      "  2112250075       0.00      0.00      0.00         0\n",
      "  2120200078       0.00      0.00      0.00         0\n",
      "  2120300077       0.95      1.00      0.98        20\n",
      "  2130100088       0.00      0.00      0.00         0\n",
      "  2130200085       0.00      0.00      0.00         0\n",
      "  2130650084       0.50      1.00      0.67         1\n",
      "  2130650086       1.00      0.80      0.89         5\n",
      "  2132250079       0.89      1.00      0.94        16\n",
      "  2132250080       1.00      1.00      1.00        12\n",
      "  2132250081       1.00      1.00      1.00        11\n",
      "  2132250082       1.00      1.00      1.00         8\n",
      "  2132250083       1.00      0.60      0.75         5\n",
      "  2132250087       1.00      1.00      1.00         6\n",
      "  2132250089       1.00      0.95      0.98        22\n",
      "  2140300101       0.00      0.00      0.00         0\n",
      "  2140450104       0.00      0.00      0.00         0\n",
      "  2140650103       0.00      0.00      0.00         0\n",
      "  2142250102       0.00      0.00      0.00         0\n",
      "\n",
      "   micro avg       0.92      0.93      0.92       941\n",
      "   macro avg       0.59      0.60      0.59       941\n",
      "weighted avg       0.93      0.93      0.92       941\n",
      " samples avg       0.93      0.94      0.93       941\n",
      "\n",
      "Product Name: 3星线【c-c】25W双C线【2米线】 -> Predicted Codes: 1110020026, 2110250076\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\miniconda3\\envs\\s\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "d:\\miniconda3\\envs\\s\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "d:\\miniconda3\\envs\\s\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "d:\\miniconda3\\envs\\s\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import  HashingVectorizer\n",
    "from sklearn.feature_extraction.text import  CountVectorizer\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from collections import Counter\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import jieba\n",
    "import re\n",
    "# 初始化参数\n",
    "df = pd.read_excel('demo.xlsx',sheet_name='demo',usecols=[0,1])\n",
    "meta = pd.read_excel('demo.xlsx',sheet_name='meta',usecols=[0,1],dtype=str)\n",
    "\n",
    "# code  = pd.read_excel('D:\\python\\pytorch-learn\\code.xlsx',sheet_name='基础编码',usecols=[0,1,2,3,4])\n",
    "df['code'] = df['code'].apply(lambda x: x.split('-'))\n",
    "all_labels = [item for sublist in df['code'] for item in sublist]\n",
    "label_counts = Counter(all_labels)\n",
    "rare_labels = {label for label, count in label_counts.items() if count == 1}\n",
    "jieba.add_word(\"1.5米\")\n",
    "jieba.add_word(\"1米\")\n",
    "jieba.add_word(\"2米\")\n",
    "jieba.add_word(\"1.8米\")\n",
    "jieba.add_word(\"线\")\n",
    "jieba.add_word(\"100w\")\n",
    "jieba.add_word(\"88w\")\n",
    "jieba.add_word(\"66w\")\n",
    "jieba.add_word(\"67w\")\n",
    "jieba.add_word(\"80w\")\n",
    "jieba.add_word(\"200w\")\n",
    "jieba.add_word(\"60w\")\n",
    "jieba.add_word(\"45w\")\n",
    "jieba.add_word(\"cc\")\n",
    "jieba.add_word(\"120w\")\n",
    "jieba.add_word(\"88w\")\n",
    "jieba.add_word(\"荣max\")\n",
    "jieba.add_word(\"荣\")\n",
    "jieba.add_word(\"max\")\n",
    "jieba.add_word(\"30w\")\n",
    "jieba.add_word(\"65w\")\n",
    "jieba.add_word(\"op\")\n",
    "jieba.add_word(\"oppo\")\n",
    "jieba.add_word(\"mi\")\n",
    "jieba.add_word(\"mini\")\n",
    "jieba.add_word(\"mirco\")\n",
    "jieba.add_word(\"vo\")\n",
    "jieba.add_word(\"v\")\n",
    "jieba.add_word(\"240w\")\n",
    "jieba.add_word(\"pro\")\n",
    "#  '14' '15' '150' '150w' '1516' '16' '18w' '1米' '2' '200w' '20w' '22.5' '240w' '25w' '29w' '29wtypec' '2a' '2米' '3' '30w' '33w' '35w' '4' '40w'\n",
    "jieba.add_word(\"150w\")\n",
    "jieba.add_word(\"22.5w\")\n",
    "jieba.add_word(\"18w\")\n",
    "jieba.add_word(\"30w\")\n",
    "jieba.add_word(\"33w\")\n",
    "jieba.add_word(\"35w\")\n",
    "jieba.add_word(\"40w\")\n",
    "jieba.add_word(\"29w\")\n",
    "jieba.add_word(\"typec\")\n",
    "jieba.add_word(\"tpc\")\n",
    "jieba.add_word(\"大壳头\")\n",
    "jieba.add_word(\"头\")\n",
    "jieba.add_word(\"10a\")\n",
    "jieba.add_word(\"6a\")\n",
    "jieba.add_word(\"5a\")\n",
    "jieba.add_word(\"85w\")\n",
    "jieba.add_word(\"max\")\n",
    "jieba.add_word(\"usb\")\n",
    "jieba.add_word(\"d\")\n",
    "jieba.add_word(\"g\")\n",
    "jieba.add_word(\"plus\")\n",
    "jieba.add_word(\"18w\")\n",
    "jieba.add_word(\"12w\")\n",
    "jieba.add_word(\"55w\")\n",
    "jieba.add_word(\"90w\")\n",
    "jieba.add_word(\"55w\")\n",
    "jieba.add_word(\"44w\")\n",
    "jieba.add_word(\"10w\")\n",
    "jieba.add_word(\"快充头\")\n",
    "jieba.add_word(\"快充\")\n",
    "jieba.add_word(\"闪充头\")\n",
    "jieba.add_word(\"闪充\")\n",
    "jieba.add_word(\"安卓线\")\n",
    "jieba.add_word(\"安卓\")\n",
    "jieba.add_word(\"华\")\n",
    "jieba.add_word(\"双口\")\n",
    "jieba.add_word(\"c口\")\n",
    "jieba.add_word(\"单口\")\n",
    "jieba.add_word(\"华为\",freq=0)\n",
    "jieba.add_word(\"ac\")\n",
    "jieba.add_word(\"vvivo\",freq=0)\n",
    "stop_words=[\"【\", \"】\", \"+\",\"（\",\"）\",\"(\",\")\",\" \",\"*\",\",\",\"/\",\"=\",\"|\"]\n",
    "#stop_words=[]\n",
    "def chinese_tokenizer(context):\n",
    "# 使用正则表达式去除自定义停止词\n",
    "    for stop_word in stop_words:\n",
    "        context = context.replace(stop_word, ' ')\n",
    "    context = context.replace('-', '')\n",
    "    context = context.replace('vivo', 'vvivo')\n",
    "    context = context.lower()\n",
    "    return list(jieba.cut(context))\n",
    "\n",
    "\n",
    "vectorizer = CountVectorizer(\n",
    "    analyzer=chinese_tokenizer,\n",
    "    token_pattern=None,\n",
    "    lowercase=True,\n",
    "    binary=False\n",
    ")\n",
    "\n",
    "md = MultiOutputClassifier(DecisionTreeClassifier(criterion=\"entropy\",random_state=42,max_depth=None,max_features=None))\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf', vectorizer),\n",
    "    ('clf', md)\n",
    "])\n",
    "\n",
    "\n",
    "# 创建一个新的 DataFrame 来保存稀有类别的样本\n",
    "df_rare = df[df['code'].apply(lambda x: any(item in rare_labels for item in x))]\n",
    "\n",
    "# 从原始 DataFrame 中移除这些稀有类别的样本\n",
    "df_common = df.drop(df_rare.index)\n",
    "mlb = MultiLabelBinarizer()\n",
    "meta['code'] = meta['code'].apply(lambda x:[x])\n",
    "mlb.fit(meta['code'])\n",
    "#mlb.fit(df_common['code'])\n",
    "Y_common = mlb.transform(df_common['code'])\n",
    "# 拆分常见类别的数据集\n",
    "X_train_common, X_test_common, Y_train_common, Y_test_common = train_test_split(\n",
    "    df_common['name'], Y_common, test_size=0.20, random_state=45\n",
    ")\n",
    "# 将稀有类别的样本添加到训练集中\n",
    "X_train_rare = df_rare['name']\n",
    "Y_train_rare = mlb.transform(df_rare['code'])\n",
    "# print(X_train_rare.shape, Y_train_rare.shape)\n",
    "# 合并训练集\n",
    "X_train = pd.concat([meta['name'],X_train_common, X_train_rare], ignore_index=True)\n",
    "Y_train = np.vstack([mlb.transform(meta['code']),Y_train_common, Y_train_rare])\n",
    "print(X_train.shape, Y_train.shape)\n",
    "# 测试集保持不变\n",
    "X_test = X_test_common\n",
    "Y_test = Y_test_common\n",
    "#print(mlb.classes_)\n",
    "pipeline.fit(X_train, Y_train)\n",
    "Y_pred = pipeline.predict(X_test)\n",
    "print(pipeline.score(X_train, Y_train))\n",
    "print(pipeline.score(X_test, Y_test))\n",
    "\n",
    "# 打印特征名称（词汇表）\n",
    "# print(\"Feature names:\", vectorizer.get_feature_names_out())\n",
    "print(classification_report(Y_test, Y_pred, target_names=mlb.classes_))\n",
    "X_pre = [\"3星线【c-c】25W双C线【2米线】\"]\n",
    "n_pred = pipeline.predict(X_pre)\n",
    "predicted_codes = mlb.inverse_transform(n_pred)\n",
    "# print(cross_val_score(pipeline, X_train, Y_train, cv=5))\n",
    "for name, codes in zip(X_pre, predicted_codes):\n",
    "   print(f\"Product Name: {name} -> Predicted Codes: {', '.join(codes)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd93435717744bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "b57eb376934b75a0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T19:43:55.409976Z",
     "start_time": "2025-01-09T19:43:55.403958Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['华为', '【', '66W', '快充', '快充头', '+', '2米', '米线', '】']"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import jieba\n",
    "jieba.add_word(\"华为\",freq=1)\n",
    "list(jieba.cut(\"华为【66W快充头+2米线】\",HMM=True,cut_all=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2bda31feefc3c3ca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-07T13:08:11.805555Z",
     "start_time": "2025-01-07T13:08:11.791280Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature names: ['and' 'document' 'first' 'is' 'one' 'second' 'the' 'third' 'this']\n",
      "TF-IDF Matrix:\n",
      " [[0.         0.46979139 0.58028582 0.38408524 0.         0.\n",
      "  0.38408524 0.         0.38408524]\n",
      " [0.         0.6876236  0.         0.28108867 0.         0.53864762\n",
      "  0.28108867 0.         0.28108867]\n",
      " [0.51184851 0.         0.         0.26710379 0.51184851 0.\n",
      "  0.26710379 0.51184851 0.26710379]\n",
      " [0.         0.46979139 0.58028582 0.38408524 0.         0.\n",
      "  0.38408524 0.         0.38408524]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# 示例文本数据\n",
    "corpus = [\n",
    "    'This is the first document.',\n",
    "    'This document is the second document.',\n",
    "    'And this is the third one.',\n",
    "    'Is this the first document?',\n",
    "]\n",
    "# 自定义分词器函数\n",
    "def weighted_tokenizer(text):\n",
    "    # 这里简单地按空格分割，实际中应使用适当的分词工具如 jieba\n",
    "    tokens = text.split()\n",
    "    print(tokens)\n",
    "\n",
    "    \n",
    "    return tokens\n",
    "# 初始化 TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(\n",
    "   # tokenizer=weighted_tokenizer,\n",
    ")\n",
    "\n",
    "# 拟合并转换文本数据\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# 打印特征名称（词汇表）\n",
    "print(\"Feature names:\", vectorizer.get_feature_names_out())\n",
    "\n",
    "# 打印 TF-IDF 矩阵\n",
    "print(\"TF-IDF Matrix:\\n\", X.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "522477ad846cf71f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-07T13:14:41.362460Z",
     "start_time": "2025-01-07T13:14:41.345134Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.float64(0.0006126662368676564),\n",
       " np.float64(0.0002630421810210078),\n",
       " np.float64(3.778449955734512e-05),\n",
       " np.float64(0.013137388281866814))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import jieba\n",
    "# 初始化参数\n",
    "df = pd.read_excel('demo.xlsx',sheet_name='demo',usecols=[0,1])\n",
    "\n",
    "df['code'] = df['code'].apply(lambda x: x.split(','))\n",
    "all_labels = [item for sublist in df['code'] for item in sublist]\n",
    "label_counts = Counter(all_labels)\n",
    "rare_labels = {label for label, count in label_counts.items() if count == 1}\n",
    "# 稀有类别对验证没有任何帮助，直接去除\n",
    "df['code'] = df['code'].apply(lambda x: np.unique( [item for item in x if item not in rare_labels]))\n",
    "# rare_labels,np.unique(df['code'].values)\n",
    "df = df[df['code'].apply(lambda x: len(x)>0)]\n",
    "# 构建特征, 使用l2进行归一化，smooth_idf 方式分母为0\n",
    "tf = TfidfVectorizer(norm=\"l2\",smooth_idf=True)\n",
    "X = tf.fit_transform(df['name'])\n",
    "mlb = MultiLabelBinarizer()\n",
    "\n",
    "y = mlb.fit_transform(df['code'])\n",
    "\n",
    "# 方差过滤\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "X_var = VarianceThreshold().fit_transform(X)\n",
    "\n",
    "\n",
    "# 方差太小，说明特征无效\n",
    "X_pd = pd.DataFrame(X.toarray())\n",
    "X_pd.var().mean(),X_pd.var().median(),X_pd.var().min(),X_pd.var().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c74621",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.float64(0.001927978523826236),\n",
       " np.float64(0.000798134963955713),\n",
       " np.float64(4.452107091598113e-05),\n",
       " np.float64(0.023728835386231224))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 构建特征, 使用l2进行归一化，smooth_idf 方式分母为0\n",
    "import jieba\n",
    "tf = TfidfVectorizer(norm=\"l2\",smooth_idf=True,analyzer=jieba.cut)\n",
    "X = tf.fit_transform(df['name'])\n",
    "\n",
    "\n",
    "\n",
    "# 方差过滤\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "X_var = VarianceThreshold().fit_transform(X)\n",
    "\n",
    "\n",
    "# 方差变大，说明区别明显了\n",
    "X_pd = pd.DataFrame(X.toarray())\n",
    "X_pd.var().mean(),X_pd.var().median(),X_pd.var().min(),X_pd.var().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21f925fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache C:\\Users\\ADMINI~1\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.500 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(np.float64(0.01752685805104584),\n",
       " np.float64(0.001810359126497556),\n",
       " np.float64(0.0002266545784224851),\n",
       " np.float64(0.49361364371154876))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 尝试直接count， jieba 分词\n",
    "import jieba\n",
    "from sklearn.feature_extraction.text import  CountVectorizer\n",
    "stop=[\"【\", \"】\", \"+\",\"（\",\"）\",\"(\",\")\",\" \",\"*\",\",\",\"/\",\"=\",\"|\"]\n",
    "tf = CountVectorizer(analyzer=jieba.cut)\n",
    "X = tf.fit_transform(df['name'])\n",
    "\n",
    "\n",
    "\n",
    "# 方差过滤\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "X_var = VarianceThreshold().fit_transform(X)\n",
    "\n",
    "\n",
    "# 方差变大，说明区别明显了\n",
    "X_pd = pd.DataFrame(X.toarray())\n",
    "X_pd.var().mean(),X_pd.var().median(),X_pd.var().min(),X_pd.var().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1786e20c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.8341947232739537)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 尝试结果\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score,KFold\n",
    "from sklearn.tree import DecisionTreeClassifier,ExtraTreeClassifier\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier,ExtraTreesClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 假设你有 X (特征), y (标签)\n",
    "skf = MultilabelStratifiedKFold(n_splits=5)\n",
    "\n",
    "md = MultiOutputClassifier(DecisionTreeClassifier(criterion=\"entropy\",splitter=\"random\"))\n",
    "\n",
    "# 使用 cross_val_score 并指定 scoring 参数\n",
    "cross_val_score(md,X_rfe,y,cv=skf,scoring=\"accuracy\").mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f54bf02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 尝试结果\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score,KFold\n",
    "from sklearn.tree import DecisionTreeClassifier,ExtraTreeClassifier\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier,ExtraTreesClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "# 假设你有 X (特征), y (标签)\n",
    "skf = MultilabelStratifiedKFold(n_splits=5)\n",
    "\n",
    "md = MultiOutputClassifier(DecisionTreeClassifier(criterion=\"log_loss\",splitter=\"random\",min_samples_split=2,min_samples_leaf=1,max_depth=7))\n",
    "\n",
    "demo_df = pd.read_excel(\"D:/work/采购/现实仓库/demo.xlsx\",usecols=[\"商品简称\"])\n",
    "demo_df[\"商品简称\"] = demo_df[\"商品简称\"].str.replace(\"\\t\",\"\")\n",
    "\n",
    "md.fit(X_var,y)\n",
    "\n",
    "X_input = vectorizer.transform(demo_df[\"商品简称\"].values.ravel())\n",
    "n_pred = md.predict(X_input)\n",
    "predicted_codes = mlb.inverse_transform(n_pred)\n",
    "re = []\n",
    "for code in predicted_codes:\n",
    "    s = \",\".join(code)\n",
    "    re.append(s)\n",
    "demo_df['预测结果'] = re\n",
    "demo_df.to_excel(\"D:/work/采购/现实仓库/demo_.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b97e93e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[235]]\n",
      "[[300]]\n",
      "[' ' '..' '1' '1.5' '1.5米' '1.8米' '100' '100w' '10a' '10w' '120' '120w'\n",
      " '12a' '12w' '14' '14pd' '15' '150' '150w' '16' '168' '18w' '1米' '2'\n",
      " '200w' '20w' '22.5w' '240w' '25w' '29w' '2a' '2米' '3' '30w' '33' '33w'\n",
      " '35w' '3t' '4' '40' '40w' '44w' '45' '45w' '4a' '5' '55w' '5a' '5t'\n",
      " '5v2a' '5v4a' '6' '60w' '614' '61w' '65' '65w' '66' '66w' '67' '67w' '6a'\n",
      " '6t' '7' '8' '80' '80w' '814' '85w' '87w' '88' '88w' '8a' '9' '90' '90w'\n",
      " '96w' ';' 'a' 'a1181' 'a1278' 'a1286' 'a1343' 'a1369' 'a1370' 'a1398'\n",
      " 'a1424' 'a1425' 'a1465' 'a1466' 'a1502' 'a1534' 'a1706' 'a1707' 'a1708'\n",
      " 'a1932' 'a1990' 'a2141' 'a2166' 'a2363' 'a2518' 'anc' 'c' 'cc' 'cro'\n",
      " 'ctoc' 'ctoc口' 'c口' 'd' 'e' 'findx6' 'findx8' 'g' 'g66' 'h' 'iapd' 'ic'\n",
      " 'ipad' 'ipad10' 'iqoo' 'k10' 'k11' 'k11x' 'k20' 'k5' 'k50' 'k60' 'k7'\n",
      " 'k70' 'k80' 'k9' 'l' 'lightning' 'magic6' 'max' 'mi' 'mini' 'mirco'\n",
      " 'note13' 'note7' 'oc' 'op' 'oppo' 'pd' 'pd20w' 'plus' 'pro' 'r9' 'reno10'\n",
      " 'reno11' 'reno12' 'reno9' 'sam' 't' 'to' 'tpc' 'typec' 'usb' 'v' 'v120'\n",
      " 'v33' 'v3380' 'vi' 'viov' 'vo' 'vvivo' 'x20' 'x70' 'x70t' 'x9' '一个' '一体'\n",
      " '一加' '一嘉' '一条' '三星' '三条' '不' '不伤机' '专用' '两条' '个' '为主' '主动' '主推' '件' '优惠'\n",
      " '充' '充头' '充电' '充电器' '充线' '光速' '入耳式' '全' '全系列' '公' '其他' '兼容' '冲双' '冲头' '准'\n",
      " '功率' '加' '加长' '加长版' '努比亚' '半耳式' '华' '华为' '华双' '华总' '单' '单个' '单头' '单条'\n",
      " '单线' '即' '原' '双' '双口' '双头' '双引擎' '发' '发华' '口' '口红' '口线' '同' '后面' '含线'\n",
      " '品牌' '型' '备注' '大壳' '头' '头大壳' '头线' '头高功' '套' '套装' '安卓' '对公' '小' '小壳' '小数点'\n",
      " '小白头' '小米' '布丁' '平板' '平果' '开盖' '弯' '弯头线' '扁口' '手机' '接口' '推' '推全' '搭配'\n",
      " '数据' '星' '星线' '是' '显示' '普通' '条' '条一加' '条装' '极速' '标准版' '标线' '根' '梯形' '正品'\n",
      " '正常' '没有' '爆款' '电' '电源适配器' '电线' '白' '白口' '白头' '白色' '直头' '看' '秒' '竞版'\n",
      " '笔记本' '笔记本电脑' '米' '米全' '米线' '米金' '系列' '紫口线' '红米' '红线' '红色' '线' '线全' '线双'\n",
      " '线米' '线紫口' '绿口' '绿口线' '编织' '胶囊' '苹果' '荣' '荣max' '荣耀' '蓝标' '装' '装配' '请'\n",
      " '超级' '足' '转' '连' '适用' '适配器' '通用' '配' '配双' '金' '金标' '金标头' '链接' '长' '闪充'\n",
      " '降噪' '高' '高攻' '鲨' '黄' '黄口' '黑' '黑头' '黑色' '默认' '，']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(np.float64(0.016298754339515806),\n",
       " np.float64(0.0029384936241774275),\n",
       " np.float64(0.0002266545784224851),\n",
       " np.float64(0.24642095412584322))"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 尝试直接count\n",
    "import jieba\n",
    "from sklearn.feature_extraction.text import  CountVectorizer\n",
    "# 尝试清理噪音\n",
    "\n",
    "stop=[\"【\", \"】\", \"+\",\"（\",\"）\",\"(\",\")\",\"*\",\",\",\"/\",\"=\",\"|\",\"以上\",\"如果\",\"邮费\",\"\\t\",\"快充\"]\n",
    "jieba.load_userdict(\"userdict.txt\")\n",
    "def chinese_tokenizer(context):\n",
    "# 使用正则表达式去除自定义停止词\n",
    "    for stop_word in stop:\n",
    "        context = context.replace(stop_word, ' ')\n",
    "    context = context.replace('-', '')\n",
    "    context = context.replace('vivo', 'vvivo')\n",
    "    context = context.replace('编制', '编织')\n",
    "    context = context.lower()\n",
    "    re =  list(jieba.cut(context,cut_all=False))\n",
    "    # if any(word == \"单头\" for word in re):\n",
    "        # print(context)\n",
    "    return re\n",
    "\n",
    "\n",
    "vectorizer = CountVectorizer(\n",
    "    analyzer=chinese_tokenizer,\n",
    "    token_pattern=None,\n",
    "    lowercase=True,\n",
    "    binary=True\n",
    ")\n",
    "X = vectorizer.fit_transform(df['name'])\n",
    "# vocabulary = vectorizer.vocabulary_\n",
    "# vocabulary[\" \"] = 196\n",
    "# vocabulary[\"头\"] = 0\n",
    "# vocabulary[\"1\"] = 255\n",
    "# vocabulary[\"线\"] = 1\n",
    "# vectorizer = CountVectorizer(\n",
    "#     analyzer=chinese_tokenizer,\n",
    "#     token_pattern=None,\n",
    "#     lowercase=True,\n",
    "#     binary=True,\n",
    "#     vocabulary=vocabulary\n",
    "# )\n",
    "# X = vectorizer.fit_transform(df['name'])\n",
    "print(np.argwhere(vectorizer.get_feature_names_out() == \"头\"))\n",
    "\n",
    "print(np.argwhere(vectorizer.get_feature_names_out() == \"线\"))\n",
    "print(vectorizer.get_feature_names_out())\n",
    "# 方差过滤\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# pca = PCA(n_components=0.97,svd_solver=\"full\")  # 降维到2维，白化\n",
    "\n",
    "# X_dr = pca.fit_transform(X.toarray())  # 训练\n",
    "# print(X_dr.shape)  # 降维后的数据\n",
    "# # 去除噪音\n",
    "# X = pca.inverse_transform(X_dr)  # 还原数据\n",
    "# print(X.shape)  # 还原后的数据\n",
    "\n",
    "X_var = VarianceThreshold().fit_transform(X)\n",
    "\n",
    "\n",
    "# 方差变大，说明区别明显了\n",
    "X_pd = pd.DataFrame(X.toarray())\n",
    "X_pd.var().mean(),X_pd.var().median(),X_pd.var().min(),X_pd.var().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d15cadf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded succeed\n",
      "['.' '1' '1.5米' '1.5米线' '1.8米' '10' '100' '100w' '10a' '10a线' '10pro单头'\n",
      " '10pro套装' '10w' '11' '1181' '11pro单头' '11pro套装' '120' '120w' '1278'\n",
      " '12a线' '12pro单头' '12w' '135' '1398' '14' '1424' '1425' '1465' '1466'\n",
      " '14pro单头' '14pro套装' '15' '150' '1502' '150240' '150w' '1534' '16' '1706'\n",
      " '1708' '18w' '18w头' '1932' '1米' '2' '20' '200' '200w' '200w线' '200w线cc'\n",
      " '200w闪' '2141' '2166' '22.5w' '240w' '25' '25w头' '29w' '29wtypec快' '2米'\n",
      " '3' '30' '30w' '3380' '33w' '35w' '3星cc' '3星线' '4' '40w' '44w' '44w闪充线'\n",
      " '45w' '45wl型' '45wt型' '45w头' '4a闪充线' '5' '50' '50pro单头' '50pro套装' '55w'\n",
      " '55w金' '55w闪充线' '5a' '5a紫口线' '5a线' '60' '60pro单头' '60pro套装' '60w' '60wl型'\n",
      " '60wt型' '61' '614' '614线usb' '61wtypec' '61wtypec快' '65' '65w' '65wcc线'\n",
      " '65w闪充线' '66' '66w' '66w闪充' '67' '67w' '67w头' '6a' '6a双c线' '6a线'\n",
      " '6a线typec快充' '6a金' '7' '70' '70pro单头' '70pro套装' '8' '80' '80w' '814'\n",
      " '85w' '85wl型' '85wt型' '85w套' '87' '87wtypec' '87wtypec快' '88w' '88w双口'\n",
      " '88w闪充线' '8pro白色' '90' '90w' '96' '96wtypec' '96wtypec快' 'a' 'a线' 'c'\n",
      " 'cc' 'cc线' 'ctoc口' 'ctoc口线' 'ctoc闪充' 'c口' 'c口线' 'c快' 'd' 'e' 'findx6'\n",
      " 'findx6pro单头' 'findx6pro套装' 'findx6pro装' 'findx8' 'g' 'h66' 'ipad'\n",
      " 'ipadmini' 'ipad套' 'iqoo' 'k' 'k7' 'l型' 'max' 'mi' 'mi33' 'mi45' 'mi67w金'\n",
      " 'mi90' 'mini4' 'mirco' 'mirco安卓' 'mirco安卓1' 'mirco安卓2' 'mi米' 'note7' 'o'\n",
      " 'op' 'op100' 'op120' 'op150' 'op240' 'op33' 'op65' 'op67w头' 'op67w头d'\n",
      " 'op80' 'oppo' 'oppo1' 'oppo10' 'oppo100' 'oppo120' 'oppo2' 'oppo33'\n",
      " 'oppo65' 'oppo67' 'oppo6a闪充线' 'oppo80' 'oppo8a闪充线' 'oppok9' 'oppo双c线'\n",
      " 'oppo线' 'oppo线cc' 'oppo绿口' 'oppo闪充' 'oppo闪充线' 'oppo黄口' 'opr9安卓绿口线'\n",
      " 'op白口线' 'op绿口线' 'op黄口' 'pd' 'pd快' 'pd线' 'pd线pd快' 'plus闪充线' 'pro' 'reno'\n",
      " 'reno9' 'reno9pro单头' 'reno9pro套装' 'sam星' 'sb' 'sbc' 'sb口线' 'sb线' 't'\n",
      " 'typec' 'typec1' 'typec双引擎' 'typec快' 'typec转typec闪充线' 'typec闪充线' 't型'\n",
      " 't闪' 'u' 'usb' 'usb1' 'usbc' 'usbc公' 'usb单线' 'usb口' 'usb线' 'v' 'v2'\n",
      " 'v44w头' 'v55' 'vivo' 'vivo120' 'vivo120w线usbc' 'vivo双tpc快' 'vivo快' 'vo'\n",
      " 'vo120' 'vo33' 'vooc闪' 'vooc闪充线' 'vo胶囊线' 'vo胶囊线typec闪' 'w' 'w原ic头' 'w头'\n",
      " 'w头c口' 'w头d' 'w头g' 'w头usb90' 'w快' 'w线' 'w线typec' 'w线typec闪充线' 'w金' 'w闪'\n",
      " 'w闪充' 'x' 'x9plus' '一' '一个' '一体' '一体tpc口' '一加usb红线' '一嘉65' '一嘉usb口'\n",
      " '一嘉红线' '三' '三星' '三星cc' '三星cc2' '不' '专用' '两' '为' '为主' '主' '件' '伤机' '充'\n",
      " '充1' '充2' '充c' '充充' '充双' '充头' '充安' '充电' '充电头' '充电线' '充线' '充胶' '光速' '全'\n",
      " '公双口' '兼容' '冲双tpc线' '冲头' '准' '功' '加' '加65' '加长' '加长typec双引擎' '加长版' '努比亚'\n",
      " '华' '华5' '华5a线华' '华6' '华66' '华6a线' '华6a线cc' '华6a线typec快充' '华6a线typec闪充线'\n",
      " '华6a线华' '华88' '华max' '华max66' '华maxg' '华maxg88' '华max原ic头' '华max原ic头g'\n",
      " '华为' '华双' '卓' '单个' '单个头' '单个快' '单头' '单头ctoc口' '单头c口' '单条' '单线' '原ic头' '双'\n",
      " '双c' '双c口线' '双c线' '双tpc' '双tpc快' '双tpc线' '双typec' '双typec闪充线' '双口'\n",
      " '双头typec' '双引擎' '发' '发mi67' '发oppo线' '发oppo黄口线' '发pd线' '发v' '发华' '发平'\n",
      " '口头' '口头d' '口头g' '后面' '含' '嘉' '囊线' '壳' '壳d' '壳g' '大' '大壳' '大壳g' '大壳头' '头'\n",
      " '头线' '头闪' '套' '套装' '安卓' '安卓1' '安卓2' '安卓micro' '安卓micro接口' '安卓micro通用'\n",
      " '安卓快' '安卓梯' '安卓线' '对' '小' '小米' '小米cc9' '小米金' '平板' '平果' '平果pd线' '弯头' '弯头线'\n",
      " '形micro线' '快' '快充' '快充头' '快充线' '手机' '接口' '接口线' '推' '推g' '推全' '数据线'\n",
      " '数据线oppo秒' '数据线oppo超级闪充线' '数据线oppo闪充线' '数据线华' '数点' '是' '显示' '普通' '条' '极速'\n",
      " '果u' '标g' '标准版' '标头' '标快' '标线' '标线电' '梯形' '正品' '正常' '没有' '爆款' '电器' '电头'\n",
      " '电线' '电脑' '白' '白口' '白头' '白色' '直头' '看' '秒' '竞版' '笔记本' '米' '米55' '米67w头'\n",
      " '米金' '系列' '紫口' '紫口线' '紫口线typec' '紫口线typec快充' '红' '红线' '红线cc' '红线typec闪充线'\n",
      " '红色' '线' '线ctoc口' '绿口线' '编制线' '编织' '编织线' '胶囊' '胶囊线typec闪' '胶囊线typec闪充'\n",
      " '苹果' '苹果iapd头' '荣max' '荣max66' '荣耀' '蓝标' '装' '装t型' '装配u' '超级' '超级闪充'\n",
      " '超级闪充线' '适用' '适配器' '通用' '配' '金' '金标' '金标线' '闪' '闪充' '闪充u' '闪充头' '闪充线' '高'\n",
      " '高攻' '黄' '黄口' '黄口线' '黑头' '黑色' '黑色线' '黑鲨' '黑鲨typec闪充线']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(np.float64(0.0116447168528152),\n",
       " np.float64(0.00240218435886626),\n",
       " np.float64(0.00034387895460793956),\n",
       " np.float64(0.24993269104741894))"
      ]
     },
     "execution_count": 332,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 尝试直接count\n",
    "import thulac\n",
    "from sklearn.feature_extraction.text import  CountVectorizer\n",
    "stop=[\"【\", \"】\", \"+\",\"（\",\"）\",\"(\",\")\",\" \",\"*\",\",\",\"/\",\"=\",\"|\",\"以上\",\"如果\",\"邮费\"]\n",
    "thu1 = thulac.thulac(seg_only=True,user_dict=\"userdict.txt\")  #默认模式\n",
    "# text = thu1.cut(\"v120wd【c口】头+1.5米ctoc口\", text=False)  #进行一句话分词\n",
    "\n",
    "# jieba.load_userdict(\"userdict.txt\")\n",
    "def chinese_tokenizer(context):\n",
    "# 使用正则表达式去除自定义停止词\n",
    "    for stop_word in stop:\n",
    "        context = context.replace(stop_word, ' ')\n",
    "    context = context.replace('-', '')\n",
    "    # context = context.replace('vivo', 'vvivo')\n",
    "    context = context.lower()\n",
    "    return list(thu1.cut(context,text=True).split(\" \"))\n",
    "\n",
    "\n",
    "vectorizer = CountVectorizer(\n",
    "    analyzer=chinese_tokenizer,\n",
    "    token_pattern=None,\n",
    "    lowercase=True,\n",
    "    binary=True\n",
    ")\n",
    "X = vectorizer.fit_transform(df['name'])\n",
    "print(vectorizer.get_feature_names_out())\n",
    "# 方差过滤\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "\n",
    "X_var = VarianceThreshold().fit_transform(X)\n",
    "\n",
    "\n",
    "# 方差变大，说明区别明显了\n",
    "X_pd = pd.DataFrame(X.toarray())\n",
    "X_pd.var().mean(),X_pd.var().median(),X_pd.var().min(),X_pd.var().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "id": "27827944",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2908, 289)\n",
      "(2908, 109)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(np.float64(0.048344212138318354),\n",
       " np.float64(0.025134393147688488),\n",
       " np.float64(0.0006875213223879122),\n",
       " np.float64(0.24461788624810052))"
      ]
     },
     "execution_count": 534,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 尝试嵌入法，下过变差了，threshold=0.0005 差不多，不能过大，缩减到96个特征  1/219 ,每个特征的重要性大概 0.005\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "print(X_var.shape)\n",
    "X_embedded = SelectFromModel(DecisionTreeClassifier(criterion=\"entropy\",random_state=42), threshold=0.0004).fit_transform(X, y)\n",
    "print(X_embedded.shape)\n",
    "X_embedded = VarianceThreshold().fit_transform(X_embedded)\n",
    "\n",
    "\n",
    "# 方差虽然变大了，但是特征少了\n",
    "X_pd = pd.DataFrame(X_embedded.toarray())\n",
    "X_pd.var().mean(),X_pd.var().median(),X_pd.var().min(),X_pd.var().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "80675833",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "only integer scalar arrays can be converted to a scalar index",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[46]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      8\u001b[39m rfe = RFE(rfc, n_features_to_select=\u001b[32m90\u001b[39m)\n\u001b[32m      9\u001b[39m X_rfe = rfe.fit_transform(X, y)\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m rfe.ranking_,\u001b[43mvectorizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_feature_names_out\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43mrfe\u001b[49m\u001b[43m.\u001b[49m\u001b[43msupport_\u001b[49m\u001b[43m]\u001b[49m\n",
      "\u001b[31mTypeError\u001b[39m: only integer scalar arrays can be converted to a scalar index"
     ]
    }
   ],
   "source": [
    "# 尝试包装法\n",
    "# 包装法 , 逻辑回归优先使用嵌入法，svm 优先使用包装法\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "rfc = DecisionTreeClassifier(criterion=\"entropy\",random_state=42)\n",
    "rfe = RFE(rfc, n_features_to_select=90)\n",
    "X_rfe = rfe.fit_transform(X, y)\n",
    "rfe.ranking_,vectorizer.get_feature_names_out()[:rfe.support_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "id": "ca504317",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['单', '头', '线']\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[401]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlist\u001b[39m(r))\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mthulac\u001b[39;00m   \n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m thu1 = \u001b[43mthulac\u001b[49m\u001b[43m.\u001b[49m\u001b[43mthulac\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseg_only\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43muser_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muserdict.txt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m#默认模式\u001b[39;00m\n\u001b[32m      9\u001b[39m text = thu1.cut(\u001b[33m\"\u001b[39m\u001b[33mv120wd【c口】头+1.5米ctoc口\u001b[39m\u001b[33m\"\u001b[39m, text=\u001b[38;5;28;01mFalse\u001b[39;00m)  \u001b[38;5;66;03m#进行一句话分词\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;28mprint\u001b[39m(text)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\miniconda3\\envs\\s\\Lib\\site-packages\\thulac\\__init__.py:52\u001b[39m, in \u001b[36mthulac.__init__\u001b[39m\u001b[34m(self, user_dict, model_path, T2S, seg_only, filt, max_length, deli, rm_space)\u001b[39m\n\u001b[32m     49\u001b[39m \u001b[38;5;28mself\u001b[39m.__so = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m(\u001b[38;5;28mself\u001b[39m.__user_specified_dict_name):\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m     \u001b[38;5;28mself\u001b[39m.__userDict = \u001b[43mPostprocesser\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__user_specified_dict_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muw\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m(\u001b[38;5;28mself\u001b[39m.__use_filter):\n\u001b[32m     54\u001b[39m     \u001b[38;5;28mself\u001b[39m.__myfilter = Filter((\u001b[38;5;28mself\u001b[39m.__prefix+\u001b[33m\"\u001b[39m\u001b[33mxu.dat\u001b[39m\u001b[33m\"\u001b[39m), (\u001b[38;5;28mself\u001b[39m.__prefix+\u001b[33m\"\u001b[39m\u001b[33mtime.dat\u001b[39m\u001b[33m\"\u001b[39m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\miniconda3\\envs\\s\\Lib\\site-packages\\thulac\\manage\\Postprocesser.py:20\u001b[39m, in \u001b[36mPostprocesser.__init__\u001b[39m\u001b[34m(self, filename, tag, isTxt)\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, line \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(f):\n\u001b[32m     19\u001b[39m     line = line.split()\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m     lexicon.append([decode(\u001b[43mline\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m), i])\n\u001b[32m     21\u001b[39m f.close()\n\u001b[32m     22\u001b[39m dm = DATMaker()\n",
      "\u001b[31mIndexError\u001b[39m: list index out of range"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "jieba.load_userdict(\"userdict.txt\")\n",
    "r= jieba.cut(\"单头线\",cut_all=False,HMM=False,use_paddle=True)\n",
    "print(list(r))\n",
    "\n",
    "import thulac   \n",
    "\n",
    "thu1 = thulac.thulac(seg_only=True,user_dict=\"userdict.txt\")  #默认模式\n",
    "text = thu1.cut(\"v120wd【c口】头+1.5米ctoc口\", text=False)  #进行一句话分词\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b305e158",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(np.str_('华6a线-1.5米'),)]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 尝试结果\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score,KFold\n",
    "from sklearn.tree import DecisionTreeClassifier,ExtraTreeClassifier\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier,ExtraTreesClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "# 假设你有 X (特征), y (标签)\n",
    "skf = MultilabelStratifiedKFold(n_splits=5)\n",
    "\n",
    "md = MultiOutputClassifier(DecisionTreeClassifier(criterion=\"entropy\",splitter=\"random\"))\n",
    "\n",
    "\n",
    "md.fit(X_var,y)\n",
    "feature_names = np.array(vectorizer.get_feature_names_out())\n",
    "# mask = np.array(rfe.support_)\n",
    "# print(feature_names[mask])\n",
    "X_input = vectorizer.transform([\"华66双口头【66w快充头+1.5米线】\"])\n",
    "# X_input = X_input[:,rfe.support_]\n",
    "n_pred = md.predict(X_input)\n",
    "predicted_codes = mlb.inverse_transform(n_pred)\n",
    "predicted_codes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "751b34f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['华', '66', '双口', '头', '【', '66W', '快充', '头', '+', '1.5米', '线', '】']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import jieba\n",
    "# str = \"华66W双口头【A+C】G配双C线【66W双口头+1.5米线+2米双C线】\"\n",
    "str = \"华66双口头【66W快充头+1.5米线】\"\n",
    "jieba.load_userdict(\"userdict.txt\")\n",
    "[*jieba.cut(str,cut_all=False)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "s",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
